{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b319e269",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-23T08:24:06.601268Z",
     "iopub.status.busy": "2024-05-23T08:24:06.600958Z",
     "iopub.status.idle": "2024-05-23T08:24:06.616959Z",
     "shell.execute_reply": "2024-05-23T08:24:06.616274Z"
    },
    "papermill": {
     "duration": 0.022214,
     "end_time": "2024-05-23T08:24:06.618881",
     "exception": false,
     "start_time": "2024-05-23T08:24:06.596667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('''\n",
    "l='depth_2'\n",
    "k='depth_1'\n",
    "j='depth_0'\n",
    "i='df_base'\n",
    "h='parquet_files'\n",
    "g='/kaggle/input/home-credit-credit-risk-model-stability'\n",
    "X='category'\n",
    "W='D'\n",
    "V=str\n",
    "R=None\n",
    "Q='WEEK_NUM'\n",
    "N='date_decision'\n",
    "J='target'\n",
    "H='case_id'\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess,os,gc\n",
    "from glob import glob\n",
    "import numpy as A,pandas as Y,polars as B\n",
    "from datetime import datetime\n",
    "import seaborn as y,matplotlib.pyplot as z,joblib as m,warnings as n\n",
    "n.filterwarnings('ignore')\n",
    "S=g\n",
    "from sklearn.model_selection import TimeSeriesSplit,GroupKFold,StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator,RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as o\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "class O:\n",
    "\tdef set_table_dtypes(A):\n",
    "\t\tfor C in A.columns:\n",
    "\t\t\tif C in[H,Q,'num_group1','num_group2']:A=A.with_columns(B.col(C).cast(B.Int64))\n",
    "\t\t\telif C in[N]:A=A.with_columns(B.col(C).cast(B.Date))\n",
    "\t\t\telif C[-1]in('P','A'):A=A.with_columns(B.col(C).cast(B.Float64))\n",
    "\t\t\telif C[-1]in('M',):A=A.with_columns(B.col(C).cast(B.String))\n",
    "\t\t\telif C[-1]in(W,):A=A.with_columns(B.col(C).cast(B.Date))\n",
    "\t\treturn A\n",
    "\tdef handle_dates(A):\n",
    "\t\tfor C in A.columns:\n",
    "\t\t\tif C[-1]in(W,):A=A.with_columns(B.col(C)-B.col(N));A=A.with_columns(B.col(C).dt.total_days())\n",
    "\t\tA=A.drop(N,'MONTH');return A\n",
    "\tdef filter_cols(A):\n",
    "\t\tfor C in A.columns:\n",
    "\t\t\tif(C not in[J,H,Q])&(A[C].dtype==B.String):\n",
    "\t\t\t\tD=A[C].n_unique()\n",
    "\t\t\t\tif(D==1)|(D>200):A=A.drop(C)\n",
    "\t\treturn A\n",
    "class I:\n",
    "\tdef num_expr(A):C=[A for A in A.columns if A[-1]in('P','A')];D=[B.max(A).alias(f\"max_{A}\")for A in C];return D\n",
    "\tdef date_expr(A):C=[A for A in A.columns if A[-1]in W];D=[B.max(A).alias(f\"max_{A}\")for A in C];return D\n",
    "\tdef str_expr(A):C=[A for A in A.columns if A[-1]in('M',)];D=[B.max(A).alias(f\"max_{A}\")for A in C];return D\n",
    "\tdef other_expr(A):C=[A for A in A.columns if A[-1]in('T','L')];D=[B.max(A).alias(f\"max_{A}\")for A in C];return D\n",
    "\tdef count_expr(A):C=[A for A in A.columns if'num_group'in A];D=[B.max(A).alias(f\"max_{A}\")for A in C];return D\n",
    "\tdef get_exprs(A):B=I.num_expr(A)+I.date_expr(A)+I.str_expr(A)+I.other_expr(A)+I.count_expr(A);return B\n",
    "def C(path,depth=R):\n",
    "\tA=B.read_parquet(path);A=A.pipe(O.set_table_dtypes)\n",
    "\tif depth in[1,2]:A=A.group_by(H).agg(I.get_exprs(A))\n",
    "\treturn A\n",
    "def K(regex_path,depth=R):\n",
    "\tC=[]\n",
    "\tfor D in glob(V(regex_path)):\n",
    "\t\tA=B.read_parquet(D);A=A.pipe(O.set_table_dtypes)\n",
    "\t\tif depth in[1,2]:A=A.group_by(H).agg(I.get_exprs(A))\n",
    "\t\tC.append(A)\n",
    "\tA=B.concat(C,how='vertical_relaxed');A=A.unique(subset=[H]);return A\n",
    "def Z(df_base,depth_0,depth_1,depth_2):\n",
    "\tA=df_base;A=A.with_columns(month_decision=B.col(N).dt.month(),weekday_decision=B.col(N).dt.weekday())\n",
    "\tfor(C,D)in enumerate(depth_0+depth_1+depth_2):A=A.join(D,how='left',on=H,suffix=f\"_{C}\")\n",
    "\tA=A.pipe(O.handle_dates);return A\n",
    "def a(df_data,cat_cols=R):\n",
    "\tB=cat_cols;A=df_data;A=A.to_pandas()\n",
    "\tif B is R:B=list(A.select_dtypes('object').columns)\n",
    "\tA[B]=A[B].astype(X);return A,B\n",
    "def T(df):\n",
    "\tB=df;G=B.memory_usage().sum()/1024**2\n",
    "\tfor C in B.columns:\n",
    "\t\tF=B[C].dtype\n",
    "\t\tif V(F)==X:continue\n",
    "\t\tif F!=object:\n",
    "\t\t\tD=B[C].min();E=B[C].max()\n",
    "\t\t\tif V(F)[:3]=='int':\n",
    "\t\t\t\tif D>A.iinfo(A.int8).min and E<A.iinfo(A.int8).max:B[C]=B[C].astype(A.int8)\n",
    "\t\t\t\telif D>A.iinfo(A.int16).min and E<A.iinfo(A.int16).max:B[C]=B[C].astype(A.int16)\n",
    "\t\t\t\telif D>A.iinfo(A.int32).min and E<A.iinfo(A.int32).max:B[C]=B[C].astype(A.int32)\n",
    "\t\t\t\telif D>A.iinfo(A.int64).min and E<A.iinfo(A.int64).max:B[C]=B[C].astype(A.int64)\n",
    "\t\t\telif D>A.finfo(A.float16).min and E<A.finfo(A.float16).max:B[C]=B[C].astype(A.float16)\n",
    "\t\t\telif D>A.finfo(A.float32).min and E<A.finfo(A.float32).max:B[C]=B[C].astype(A.float32)\n",
    "\t\t\telse:B[C]=B[C].astype(A.float64)\n",
    "\t\telse:continue\n",
    "\tH=B.memory_usage().sum()/1024**2;return B\n",
    "S=Path(g)\n",
    "E=S/h/'train'\n",
    "F=S/h/'test'\n",
    "L={i:C(E/'train_base.parquet'),j:[C(E/'train_static_cb_0.parquet'),K(E/'train_static_0_*.parquet')],k:[K(E/'train_applprev_1_*.parquet',1),C(E/'train_tax_registry_a_1.parquet',1),C(E/'train_tax_registry_b_1.parquet',1),C(E/'train_tax_registry_c_1.parquet',1),K(E/'train_credit_bureau_a_1_*.parquet',1),C(E/'train_credit_bureau_b_1.parquet',1),C(E/'train_other_1.parquet',1),C(E/'train_person_1.parquet',1),C(E/'train_deposit_1.parquet',1),C(E/'train_debitcard_1.parquet',1)],l:[C(E/'train_credit_bureau_b_2.parquet',2)]}\n",
    "D=Z(**L)\n",
    "del L\n",
    "gc.collect()\n",
    "D=D.pipe(O.filter_cols)\n",
    "D,p=a(D)\n",
    "D=T(D)\n",
    "b=D.select_dtypes(exclude=X).columns\n",
    "from itertools import combinations,permutations\n",
    "c=D[b].isna()\n",
    "P={}\n",
    "for U in b:\n",
    "\td=c[U].sum()\n",
    "\ttry:P[d].append(U)\n",
    "\texcept:P[d]=[U]\n",
    "del c\n",
    "A0=gc.collect()\n",
    "def q(grps):\n",
    "\tA=[]\n",
    "\tfor B in grps:\n",
    "\t\tC=0;E=B[0]\n",
    "\t\tfor F in B:\n",
    "\t\t\tG=D[F].nunique()\n",
    "\t\t\tif G>C:C=G;E=F\n",
    "\t\tA.append(E)\n",
    "\treturn A\n",
    "def r(matrix,threshold=.8):\n",
    "\tD=matrix;H=D.corr();E=[];A=list(D.columns)\n",
    "\twhile A:\n",
    "\t\tB=A.pop(0);F=[B];G=[B]\n",
    "\t\tfor C in A:\n",
    "\t\t\tif H.loc[B,C]>=threshold:F.append(C);G.append(C)\n",
    "\t\tE.append(F);A=[A for A in A if A not in G]\n",
    "\treturn E\n",
    "M=[]\n",
    "for(s,e)in P.items():\n",
    "\tif len(e)>1:t=P[s];u=r(D[t],threshold=.8);v=q(u);M=M+v\n",
    "\telse:M=M+e\n",
    "D=D[M]\n",
    "L={i:C(F/'test_base.parquet'),j:[C(F/'test_static_cb_0.parquet'),K(F/'test_static_0_*.parquet')],k:[K(F/'test_applprev_1_*.parquet',1),C(F/'test_tax_registry_a_1.parquet',1),C(F/'test_tax_registry_b_1.parquet',1),C(F/'test_tax_registry_c_1.parquet',1),K(F/'test_credit_bureau_a_1_*.parquet',1),C(F/'test_credit_bureau_b_1.parquet',1),C(F/'test_other_1.parquet',1),C(F/'test_person_1.parquet',1),C(F/'test_deposit_1.parquet',1),C(F/'test_debitcard_1.parquet',1)],l:[C(F/'test_credit_bureau_b_2.parquet',2)]}\n",
    "G=Z(**L)\n",
    "del L\n",
    "gc.collect()\n",
    "G=G.select([A for A in D.columns if A!=J])\n",
    "G,p=a(G)\n",
    "G=T(G)\n",
    "gc.collect()\n",
    "D[J]=0\n",
    "G[J]=1\n",
    "D=Y.concat([D,G])\n",
    "D=T(D)\n",
    "w=D[J]\n",
    "D=D.drop(columns=[J,H,Q])\n",
    "f=o.LGBMClassifier()\n",
    "f.fit(D,w)\n",
    "G=G.drop(columns=[Q,J])\n",
    "G=G.set_index(H)\n",
    "x=Y.Series(f.predict_proba(G)[:,1],index=G.index)\n",
    "m.dump(x,'y.pkl')\n",
    "''', file=open(\"y.py\", \"w\"), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ead2dcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T08:24:06.624716Z",
     "iopub.status.busy": "2024-05-23T08:24:06.624484Z",
     "iopub.status.idle": "2024-05-23T08:28:25.686732Z",
     "shell.execute_reply": "2024-05-23T08:28:25.685516Z"
    },
    "papermill": {
     "duration": 259.06799,
     "end_time": "2024-05-23T08:28:25.689390",
     "exception": false,
     "start_time": "2024-05-23T08:24:06.621400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python y.py\n",
    "!rm y.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dd75ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T08:28:25.696644Z",
     "iopub.status.busy": "2024-05-23T08:28:25.696333Z",
     "iopub.status.idle": "2024-05-23T08:31:32.067200Z",
     "shell.execute_reply": "2024-05-23T08:31:32.065852Z"
    },
    "papermill": {
     "duration": 186.378619,
     "end_time": "2024-05-23T08:31:32.070817",
     "exception": true,
     "start_time": "2024-05-23T08:28:25.692198",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Labels variable is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 365\u001b[0m\n\u001b[1;32m    354\u001b[0m X_train, y_train, week_num_train \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    355\u001b[0m     X\u001b[38;5;241m.\u001b[39miloc[idx_train],\n\u001b[1;32m    356\u001b[0m     y\u001b[38;5;241m.\u001b[39miloc[idx_train],\n\u001b[1;32m    357\u001b[0m     week_num\u001b[38;5;241m.\u001b[39miloc[idx_train],\n\u001b[1;32m    358\u001b[0m )\n\u001b[1;32m    359\u001b[0m X_valid, y_valid, week_num_valid \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    360\u001b[0m     X\u001b[38;5;241m.\u001b[39miloc[idx_valid],\n\u001b[1;32m    361\u001b[0m     y\u001b[38;5;241m.\u001b[39miloc[idx_valid],\n\u001b[1;32m    362\u001b[0m     week_num\u001b[38;5;241m.\u001b[39miloc[idx_valid],\n\u001b[1;32m    363\u001b[0m )\n\u001b[0;32m--> 365\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweek_num_train\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m val_pool \u001b[38;5;241m=\u001b[39m Pool(\n\u001b[1;32m    369\u001b[0m     data\u001b[38;5;241m=\u001b[39mX_valid, label\u001b[38;5;241m=\u001b[39my_valid, cat_features\u001b[38;5;241m=\u001b[39mcat_cols, weight\u001b[38;5;241m=\u001b[39mweek_num_valid\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m cat_boost_clf \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\n\u001b[1;32m    373\u001b[0m     best_model_min_trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1200\u001b[39m,\n\u001b[1;32m    374\u001b[0m     boosting_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    384\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:844\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m             )\n\u001b[0;32m--> 844\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_can_be_none:\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:1421\u001b[0m, in \u001b[0;36mPool._init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_label_type(label)\n\u001b[0;32m-> 1421\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_label_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_if_pandas_to_numpy(label)\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(label)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/catboost/core.py:958\u001b[0m, in \u001b[0;36mPool._check_label_empty\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03mCheck label is not empty.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(label) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels variable is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mCatBoostError\u001b[0m: Labels variable is empty."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "from itertools import combinations, permutations\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from glob import glob\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from typing import Any\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n",
    "    int_types = [\n",
    "        pl.Int8,\n",
    "        pl.Int16,\n",
    "        pl.Int32,\n",
    "        pl.Int64,\n",
    "        pl.UInt8,\n",
    "        pl.UInt16,\n",
    "        pl.UInt32,\n",
    "        pl.UInt64,\n",
    "    ]\n",
    "    float_types = [pl.Float32, pl.Float64]\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in int_types + float_types:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if c_min is not None and c_max is not None:\n",
    "                if col_type in int_types:\n",
    "                    if c_min >= 0:\n",
    "                        if (\n",
    "                            c_min >= np.iinfo(np.uint8).min\n",
    "                            and c_max <= np.iinfo(np.uint8).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.UInt8))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.uint16).min\n",
    "                            and c_max <= np.iinfo(np.uint16).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.UInt16))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.uint32).min\n",
    "                            and c_max <= np.iinfo(np.uint32).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.UInt32))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.uint64).min\n",
    "                            and c_max <= np.iinfo(np.uint64).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.UInt64))\n",
    "                    else:\n",
    "                        if (\n",
    "                            c_min >= np.iinfo(np.int8).min\n",
    "                            and c_max <= np.iinfo(np.int8).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.int16).min\n",
    "                            and c_max <= np.iinfo(np.int16).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.int32).min\n",
    "                            and c_max <= np.iinfo(np.int32).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                        elif (\n",
    "                            c_min >= np.iinfo(np.int64).min\n",
    "                            and c_max <= np.iinfo(np.int64).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                elif col_type in float_types:\n",
    "                    if (\n",
    "                        c_min > np.finfo(np.float32).min\n",
    "                        and c_max < np.finfo(np.float32).max\n",
    "                    ):\n",
    "                        df = df.with_columns(df[col].cast(pl.Float32))\n",
    "    return df\n",
    "\n",
    "def to_pandas(\n",
    "    df: pl.DataFrame, cat_cols: list[str] = None\n",
    ") -> (pd.DataFrame, list[str]):\n",
    "    df: pd.DataFrame = df.to_pandas()\n",
    "\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df.select_dtypes(\"object\").columns)\n",
    "\n",
    "    df[cat_cols] = df[cat_cols].astype(\"str\")\n",
    "\n",
    "    return df, cat_cols\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        cols: list[str] = [\n",
    "            col\n",
    "            for col in df.columns\n",
    "            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n",
    "        ]\n",
    "\n",
    "        expr_max: list[pl.Series] = [\n",
    "            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
    "\n",
    "        expr_mean: list[pl.Series] = [\n",
    "            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_mean\n",
    "\n",
    "    @staticmethod\n",
    "    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n",
    "\n",
    "        expr_mean: list[pl.Series] = [\n",
    "            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n",
    "        ]\n",
    "\n",
    "        return expr_mean\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n",
    "        exprs = (\n",
    "            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n",
    "        )\n",
    "\n",
    "        return exprs\n",
    "\n",
    "\n",
    "def change_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if col == \"case_id\":\n",
    "            df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n",
    "        elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "            df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n",
    "        elif col == \"date_decision\" or col[-1] == \"D\":\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n",
    "        elif col[-1] in [\"P\", \"A\"]:\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "        elif col[-1] == \"M\":\n",
    "            df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "    return df\n",
    "\n",
    "\n",
    "def scan_files(glob_path: str, depth: int = None):\n",
    "    chunks = []\n",
    "    for path in glob(str(glob_path)):\n",
    "        df = pl.read_parquet(path, low_memory=True, rechunk=True)\n",
    "        df = df.pipe(change_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    del chunks\n",
    "    gc.collect()\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_dataframes(df_base, depth_0, depth_1, depth_2):\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "\n",
    "    return df_base\n",
    "\n",
    "\n",
    "def transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"riskassesment_302T\" in df.columns:\n",
    "        if df[\"riskassesment_302T\"].dtype == pl.Null:\n",
    "            df = df.with_columns(\n",
    "                [\n",
    "                    pl.Series(\n",
    "                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n",
    "                    ),\n",
    "                    pl.Series(\n",
    "                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            pct_low: pl.Series = (\n",
    "                df[\"riskassesment_302T\"]\n",
    "                .str.split(\" - \")\n",
    "                .apply(lambda x: x[0].replace(\"%\", \"\"))\n",
    "                .cast(pl.UInt8)\n",
    "            )\n",
    "            pct_high: pl.Series = (\n",
    "                df[\"riskassesment_302T\"]\n",
    "                .str.split(\" - \")\n",
    "                .apply(lambda x: x[1].replace(\"%\", \"\"))\n",
    "                .cast(pl.UInt8)\n",
    "            )\n",
    "\n",
    "            diff: pl.Series = pct_high - pct_low\n",
    "            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n",
    "\n",
    "            del pct_high, pct_low\n",
    "            gc.collect()\n",
    "\n",
    "            df = df.with_columns(\n",
    "                [\n",
    "                    diff.alias(\"riskassesment_302T_rng\"),\n",
    "                    avg.alias(\"riskassesment_302T_mean\"),\n",
    "                ]\n",
    "            )\n",
    "        df.drop(\"riskassesment_302T\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"D\"):\n",
    "            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n",
    "    df = df.rename({\"WEEK_NUM\": \"week_num\"})\n",
    "    return df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "\n",
    "def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n",
    "            null_pct = df[col].is_null().mean()\n",
    "\n",
    "            if null_pct > 0.95:\n",
    "                df = df.drop(col)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) and (df[col].dtype == pl.String):\n",
    "            n_unique = df[col].n_unique()\n",
    "\n",
    "            if (n_unique > 200) | (n_unique == 1):\n",
    "                df = df.drop(col)\n",
    "\n",
    "    return df\n",
    "\n",
    "data_store: dict = {\n",
    "    \"df_base\": scan_files(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        scan_files(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n",
    "        scan_files(TRAIN_DIR / \"train_person_2.parquet\", 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "df_train = (\n",
    "    join_dataframes(**data_store)\n",
    "    .pipe(filter_cols)\n",
    "    .pipe(transform_cols)\n",
    "    .pipe(handle_dates)\n",
    "    .pipe(reduce_memory_usage, \"df_train\")\n",
    ").to_pandas()\n",
    "    \n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "cat_cols = list(df_train.select_dtypes(\"object\").columns)\n",
    "df_train = df_train.sort_values(by=\"case_id\", ascending=True)\n",
    "df_train_columns = [col for col in df_train.columns if col != \"target\"]\n",
    "\n",
    "if pd.read_csv(ROOT / \"sample_submission.csv\").shape[0] == 10:\n",
    "    df_train = df_train.iloc[:50000]\n",
    "\n",
    "df_train = df_train[df_train[\"week_num\"] > 62]\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n",
    "y = df_train[\"target\"]\n",
    "week_num = df_train[\"week_num\"]\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "lgb_params_1 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"device\": 'gpu',\n",
    "    \"extra_trees\": True,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"l1_regularization\": 0.1,\n",
    "    \"l2_regularization\": 10,\n",
    "    \"max_depth\": 20,\n",
    "    \"metric\": \"auc\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"num_leaves\": 64,\n",
    "    \"objective\": \"binary\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "\n",
    "lgb_params_2 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"device\": 'gpu',\n",
    "    \"extra_trees\": True,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"l1_regularization\": 0.1,\n",
    "    \"l2_regularization\": 10,\n",
    "    \"max_depth\": 16,\n",
    "    \"metric\": \"auc\",\n",
    "    \"n_estimators\": 2000,\n",
    "    \"num_leaves\": 72,\n",
    "    \"objective\": \"binary\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "\n",
    "X[cat_cols] = X[cat_cols].astype(\"str\")\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=10, shuffle=False)\n",
    "for iter_idx, (idx_train, idx_valid) in enumerate(cv.split(X, y, groups=week_num)):\n",
    "    X_train, y_train, week_num_train = (\n",
    "        X.iloc[idx_train],\n",
    "        y.iloc[idx_train],\n",
    "        week_num.iloc[idx_train],\n",
    "    )\n",
    "    X_valid, y_valid, week_num_valid = (\n",
    "        X.iloc[idx_valid],\n",
    "        y.iloc[idx_valid],\n",
    "        week_num.iloc[idx_valid],\n",
    "    )\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data=X_train, label=y_train, cat_features=cat_cols, weight=week_num_train\n",
    "    )\n",
    "    val_pool = Pool(\n",
    "        data=X_valid, label=y_valid, cat_features=cat_cols, weight=week_num_valid\n",
    "    )\n",
    "\n",
    "    cat_boost_clf = CatBoostClassifier(\n",
    "        best_model_min_trees = 1200,\n",
    "        boosting_type = \"Plain\",\n",
    "        eval_metric = \"AUC\",\n",
    "        iterations = 600 if pd.read_csv(ROOT / \"sample_submission.csv\").shape[0] == 10 else 6000,\n",
    "        learning_rate = 0.05,\n",
    "        l2_leaf_reg = 10,\n",
    "        max_leaves = 64,\n",
    "        random_seed = 42,\n",
    "        task_type = \"GPU\",\n",
    "        use_best_model = True,\n",
    "        verbose = 1,\n",
    "    )\n",
    "\n",
    "    cat_boost_clf.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "\n",
    "    joblib.dump(cat_boost_clf, f\"cat_boost_fold_{iter_idx}.pkl\")\n",
    "\n",
    "    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
    "    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
    "\n",
    "    if iter_idx % 2 == 0:\n",
    "        light_gbm_clf = lgb.LGBMClassifier(**lgb_params_1)\n",
    "    else:\n",
    "        light_gbm_clf = lgb.LGBMClassifier(**lgb_params_2)\n",
    "\n",
    "    light_gbm_clf.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n",
    "    )\n",
    "    \n",
    "    joblib.dump(light_gbm_clf, f\"light_gbm_fold_{iter_idx}.pkl\")\n",
    "\n",
    "    del (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        week_num_train,\n",
    "        week_num_valid,\n",
    "        train_pool,\n",
    "        val_pool,\n",
    "        cat_boost_clf,\n",
    "        light_gbm_clf,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "del week_num, X, y\n",
    "gc.collect()\n",
    "\n",
    "data_store: dict = {\n",
    "    \"df_base\": scan_files(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        scan_files(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        scan_files(TEST_DIR / \"test_person_2.parquet\", 2),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_test: pl.DataFrame = (\n",
    "    join_dataframes(**data_store)\n",
    "    .pipe(transform_cols)\n",
    "    .pipe(handle_dates)\n",
    "    .select(df_train_columns)\n",
    "    .pipe(reduce_memory_usage, \"df_test\")\n",
    ")\n",
    "\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "fitted_models_cat = []\n",
    "for path in glob(\"cat_boost_*.pkl\"):\n",
    "    fitted_models_cat.append(joblib.load(path))\n",
    "\n",
    "\n",
    "fitted_models_lgb = []\n",
    "for path in glob(\"light_gbm_*.pkl\"):\n",
    "    fitted_models_lgb.append(joblib.load(path))\n",
    "\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "\n",
    "X_test: pd.DataFrame = df_test.drop(columns=[\"week_num\"]).set_index(\"case_id\")\n",
    "\n",
    "X_test[cat_cols] = X_test[cat_cols].astype(\"category\")\n",
    "\n",
    "\n",
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators: list[BaseEstimator]):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [\n",
    "            estimator.predict_proba(X)[:, 1]\n",
    "            if hasattr(estimator, \"predict_proba\")\n",
    "            else estimator.predict(X)\n",
    "            for estimator in self.estimators\n",
    "        ]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "ensemble_model = VotingModel(fitted_models_cat + fitted_models_lgb)\n",
    "\n",
    "del df_train_columns, fitted_models_cat, fitted_models_lgb\n",
    "gc.collect()\n",
    "\n",
    "y_pred: pd.Series = pd.Series(ensemble_model.predict_proba(X_test), index=X_test.index)\n",
    "\n",
    "df_subm: pd.DataFrame = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "df_subm[\"score\"] = y_pred\n",
    "y_train_test_pred = joblib.load(\"y.pkl\")\n",
    "condition = y_train_test_pred < 0.978\n",
    "df_subm.loc[condition, \"score\"] = (df_subm.loc[condition, \"score\"] - 0.0718).clip(0)\n",
    "\n",
    "display(df_subm)\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "\n",
    "del X_test, y_pred, df_subm\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 449.004138,
   "end_time": "2024-05-23T08:31:32.894127",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T08:24:03.889989",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
